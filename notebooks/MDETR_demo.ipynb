{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "MDETR_demo.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "3TEPUFkaKCZt"
      ],
      "toc_visible": true,
      "gpuType": "A100"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X-O-uMh8Jjpi"
      },
      "source": [
        "# MDETR - Modulated Detection for End-to-End Multi-Modal Understanding\n",
        "\n",
        "Welcome to the demo notebook for MDETR. We'll show-case detection, segmentation and question answering\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3TEPUFkaKCZt"
      },
      "source": [
        "## Preliminaries\n",
        "\n",
        "This section contains the initial boilerplate. Run it first."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pxj8xahJIvzY",
        "outputId": "38c4bf7b-52e5-4e81-f513-44b54940b2e7"
      },
      "source": [
        "! pip install timm transformers"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: timm in /usr/local/lib/python3.12/dist-packages (1.0.22)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (4.57.3)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (from timm) (2.9.0+cu126)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.12/dist-packages (from timm) (0.24.0+cu126)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.12/dist-packages (from timm) (6.0.3)\n",
            "Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.12/dist-packages (from timm) (0.36.0)\n",
            "Requirement already satisfied: safetensors in /usr/local/lib/python3.12/dist-packages (from timm) (0.7.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers) (3.20.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (25.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2025.11.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers) (2.32.4)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.22.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub->timm) (2025.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub->timm) (4.15.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface_hub->timm) (1.2.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2025.11.12)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch->timm) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch->timm) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch->timm) (3.6.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch->timm) (3.1.6)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->timm) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->timm) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch->timm) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch->timm) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch->timm) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch->timm) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch->timm) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch->timm) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch->timm) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch->timm) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch->timm) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch->timm) (3.3.20)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch->timm) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch->timm) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch->timm) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch->timm) (3.5.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from torchvision->timm) (11.3.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch->timm) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch->timm) (3.0.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YYiTH9RPKBxS"
      },
      "source": [
        "import torch\n",
        "from PIL import Image\n",
        "import requests\n",
        "import torchvision.transforms as T\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import defaultdict\n",
        "import torch.nn.functional as F\n",
        "import numpy as np\n",
        "from skimage.measure import find_contours\n",
        "\n",
        "from matplotlib import patches,  lines\n",
        "from matplotlib.patches import Polygon\n",
        "\n",
        "torch.set_grad_enabled(False);"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gdot5FYATTY2"
      },
      "source": [
        "# standard PyTorch mean-std input image normalization\n",
        "transform = T.Compose([\n",
        "    T.Resize(800),\n",
        "    T.ToTensor(),\n",
        "    T.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "# for output bounding box post-processing\n",
        "def box_cxcywh_to_xyxy(x):\n",
        "    x_c, y_c, w, h = x.unbind(1)\n",
        "    b = [(x_c - 0.5 * w), (y_c - 0.5 * h),\n",
        "         (x_c + 0.5 * w), (y_c + 0.5 * h)]\n",
        "    return torch.stack(b, dim=1)\n",
        "\n",
        "def rescale_bboxes(out_bbox, size):\n",
        "    img_w, img_h = size\n",
        "    b = box_cxcywh_to_xyxy(out_bbox)\n",
        "    b = b * torch.tensor([img_w, img_h, img_w, img_h], dtype=torch.float32)\n",
        "    return b"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DYg7tyTFVMit"
      },
      "source": [
        "# colors for visualization\n",
        "COLORS = [[0.000, 0.447, 0.741], [0.850, 0.325, 0.098], [0.929, 0.694, 0.125],\n",
        "          [0.494, 0.184, 0.556], [0.466, 0.674, 0.188], [0.301, 0.745, 0.933]]\n",
        "\n",
        "def apply_mask(image, mask, color, alpha=0.5):\n",
        "    \"\"\"Apply the given mask to the image.\n",
        "    \"\"\"\n",
        "    for c in range(3):\n",
        "        image[:, :, c] = np.where(mask == 1,\n",
        "                                  image[:, :, c] *\n",
        "                                  (1 - alpha) + alpha * color[c] * 255,\n",
        "                                  image[:, :, c])\n",
        "    return image\n",
        "\n",
        "def plot_results(pil_img, scores, boxes, labels, masks=None):\n",
        "    plt.figure(figsize=(16,10))\n",
        "    np_image = np.array(pil_img)\n",
        "    ax = plt.gca()\n",
        "    colors = COLORS * 100\n",
        "    if masks is None:\n",
        "      masks = [None for _ in range(len(scores))]\n",
        "    assert len(scores) == len(boxes) == len(labels) == len(masks)\n",
        "    for s, (xmin, ymin, xmax, ymax), l, mask, c in zip(scores, boxes.tolist(), labels, masks, colors):\n",
        "        ax.add_patch(plt.Rectangle((xmin, ymin), xmax - xmin, ymax - ymin,\n",
        "                                   fill=False, color=c, linewidth=3))\n",
        "        text = f'{l}: {s:0.2f}'\n",
        "        ax.text(xmin, ymin, text, fontsize=15, bbox=dict(facecolor='white', alpha=0.8))\n",
        "\n",
        "        if mask is None:\n",
        "          continue\n",
        "        np_image = apply_mask(np_image, mask, c)\n",
        "\n",
        "        padded_mask = np.zeros((mask.shape[0] + 2, mask.shape[1] + 2), dtype=np.uint8)\n",
        "        padded_mask[1:-1, 1:-1] = mask\n",
        "        contours = find_contours(padded_mask, 0.5)\n",
        "        for verts in contours:\n",
        "          # Subtract the padding and flip (y, x) to (x, y)\n",
        "          verts = np.fliplr(verts) - 1\n",
        "          p = Polygon(verts, facecolor=\"none\", edgecolor=c)\n",
        "          ax.add_patch(p)\n",
        "\n",
        "\n",
        "    plt.imshow(np_image)\n",
        "    plt.axis('off')\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "def add_res(results, ax, color='green'):\n",
        "    #for tt in results.values():\n",
        "    if True:\n",
        "        bboxes = results['boxes']\n",
        "        labels = results['labels']\n",
        "        scores = results['scores']\n",
        "        #keep = scores >= 0.0\n",
        "        #bboxes = bboxes[keep].tolist()\n",
        "        #labels = labels[keep].tolist()\n",
        "        #scores = scores[keep].tolist()\n",
        "    #print(torchvision.ops.box_iou(tt['boxes'].cpu().detach(), torch.as_tensor([[xmin, ymin, xmax, ymax]])))\n",
        "\n",
        "    colors = ['purple', 'yellow', 'red', 'green', 'orange', 'pink']\n",
        "\n",
        "    for i, (b, ll, ss) in enumerate(zip(bboxes, labels, scores)):\n",
        "        ax.add_patch(plt.Rectangle((b[0], b[1]), b[2] - b[0], b[3] - b[1], fill=False, color=colors[i], linewidth=3))\n",
        "        cls_name = ll if isinstance(ll,str) else CLASSES[ll]\n",
        "        text = f'{cls_name}: {ss:.2f}'\n",
        "        print(text)\n",
        "        ax.text(b[0], b[1], text, fontsize=15, bbox=dict(facecolor='white', alpha=0.8))"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KjONBQSQKK2O"
      },
      "source": [
        "## Detection\n",
        "\n",
        "In this section, we show the performance of our pre-trained model on modulated detection.\n",
        "Keep in mind that this model wasn't fine-tuned for any specific task.\n",
        "\n",
        "We load the model from torch hub"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VuRgw3zjKVwA",
        "outputId": "9bd83e84-4dce-4a81-ec35-71ad290545bb"
      },
      "source": [
        "model, postprocessor = torch.hub.load('ashkamath/mdetr:main', 'mdetr_efficientnetB5', pretrained=True, return_postprocessor=True)\n",
        "model = model.cuda()\n",
        "model.eval();"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/hub.py:335: UserWarning: You are about to download and run code from an untrusted repository. In a future release, this won't be allowed. To add the repository to your trusted list, change the command to {calling_fn}(..., trust_repo=False) and a command prompt will appear asking for an explicit confirmation of trust, or load(..., trust_repo=True), which will assume that the prompt is to be answered with 'yes'. You can also use load(..., trust_repo='check') which will only prompt for confirmation if the repo is not already trusted. This will eventually be the default behaviour\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading: \"https://github.com/ashkamath/mdetr/zipball/main\" to /root/.cache/torch/hub/main.zip\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "X5B5bDq-S10_"
      },
      "source": [
        "Next, we retrieve an image on which we wish to test the model. Here, we use an image from the validation set of COCO"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D6QjsJ48S1ZQ"
      },
      "source": [
        "url = \"http://images.cocodataset.org/val2017/000000281759.jpg\"\n",
        "im = Image.open(requests.get(url, stream=True).raw)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4jffFxiSKjs7"
      },
      "source": [
        "im"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vRw-Bhf8QTAh"
      },
      "source": [
        "def plot_inference(im, caption):\n",
        "  # mean-std normalize the input image (batch-size: 1)\n",
        "  img = transform(im).unsqueeze(0).cuda()\n",
        "\n",
        "  # propagate through the model\n",
        "  memory_cache = model(img, [caption], encode_and_save=True)\n",
        "  outputs = model(img, [caption], encode_and_save=False, memory_cache=memory_cache)\n",
        "\n",
        "  # keep only predictions with 0.7+ confidence\n",
        "  probas = 1 - outputs['pred_logits'].softmax(-1)[0, :, -1].cpu()\n",
        "  keep = (probas > 0.7).cpu()\n",
        "\n",
        "  # convert boxes from [0; 1] to image scales\n",
        "  bboxes_scaled = rescale_bboxes(outputs['pred_boxes'].cpu()[0, keep], im.size)\n",
        "\n",
        "  # Extract the text spans predicted by each box\n",
        "  positive_tokens = (outputs[\"pred_logits\"].cpu()[0, keep].softmax(-1) > 0.1).nonzero().tolist()\n",
        "  predicted_spans = defaultdict(str)\n",
        "  for tok in positive_tokens:\n",
        "    item, pos = tok\n",
        "    if pos < 255:\n",
        "        span = memory_cache[\"tokenized\"].token_to_chars(0, pos)\n",
        "        predicted_spans [item] += \" \" + caption[span.start:span.end]\n",
        "\n",
        "  labels = [predicted_spans [k] for k in sorted(list(predicted_spans .keys()))]\n",
        "  plot_results(im, probas[keep], bboxes_scaled, labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n7duNgzTW_HR"
      },
      "source": [
        "Let's first try to single out the salient objects in the image"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yOIBGSLoTgcc"
      },
      "source": [
        "plot_inference(im, \"5 people each holding an umbrella\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "79HUxU2sW7sT"
      },
      "source": [
        "We can now ask to single out specific instances"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t0vg4rneXM6Y"
      },
      "source": [
        "plot_inference(im, \"A green umbrella. A pink striped umbrella. A plain white umbrella\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-FSHFBKoT7y0"
      },
      "source": [
        "plot_inference(im, \"a flowery top. A blue dress. An orange shirt\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PiW-1KUlavnj"
      },
      "source": [
        "It can also find non-salient objects"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0iUjnHk3UfCk"
      },
      "source": [
        "plot_inference(im, \"a car. An electricity box\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lnboSb2Wa7ae"
      },
      "source": [
        "Note that this model is trained with almost no negatives: during pre-training each text query is almost always associated to an object in the image. As a result, the model is biased to try to always output a detection, even if the object is not actually there (usually by picking the salient objects).\n",
        "\n",
        "See the LVIS model for a model fine-tuned with negatives."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1GLbc1ogXdIo"
      },
      "source": [
        "plot_inference(im, \"a koala\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Tv4-AYncCSP"
      },
      "source": [
        "## Segmentation\n",
        "\n",
        "To show-case segmentation, we use our model fine-tuned on phrase-cut. The captions in phrase-cut are usually very short, and contain ony one sentence. Keep this in mind while building your own prompts."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UuttDVWVa4gt"
      },
      "source": [
        "model_pc = torch.hub.load('ashkamath/mdetr:main', 'mdetr_efficientnetB3_phrasecut', pretrained=True, return_postprocessor=False)\n",
        "model_pc = model_pc.cuda()\n",
        "model_pc.eval();"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uDLsm0mvgps2"
      },
      "source": [
        "def plot_inference_segmentation(im, caption):\n",
        "  # mean-std normalize the input image (batch-size: 1)\n",
        "  img = transform(im).unsqueeze(0).cuda()\n",
        "\n",
        "  # propagate through the model\n",
        "  outputs = model_pc(img, [caption])\n",
        "\n",
        "  # keep only predictions with 0.9+ confidence\n",
        "  probas = 1 - outputs['pred_logits'].softmax(-1)[0, :, -1].cpu()\n",
        "  keep = (probas > 0.9).cpu()\n",
        "\n",
        "  # convert boxes from [0; 1] to image scales\n",
        "  bboxes_scaled = rescale_bboxes(outputs['pred_boxes'].cpu()[0, keep], im.size)\n",
        "\n",
        "  # Interpolate masks to the correct size\n",
        "  w, h = im.size\n",
        "  masks = F.interpolate(outputs[\"pred_masks\"], size=(h, w), mode=\"bilinear\", align_corners=False)\n",
        "  masks = masks.cpu()[0, keep].sigmoid() > 0.5\n",
        "\n",
        "  tokenized = model_pc.detr.transformer.tokenizer.batch_encode_plus([caption], padding=\"longest\", return_tensors=\"pt\").to(img.device)\n",
        "\n",
        "  # Extract the text spans predicted by each box\n",
        "  positive_tokens = (outputs[\"pred_logits\"].cpu()[0, keep].softmax(-1) > 0.1).nonzero().tolist()\n",
        "  predicted_spans = defaultdict(str)\n",
        "  for tok in positive_tokens:\n",
        "    item, pos = tok\n",
        "    if pos < 255:\n",
        "        span = tokenized.token_to_chars(0, pos)\n",
        "        predicted_spans [item] += \" \" + caption[span.start:span.end]\n",
        "\n",
        "  labels = [predicted_spans [k] for k in sorted(list(predicted_spans .keys()))]\n",
        "  plot_results(im, probas[keep], bboxes_scaled, labels, masks)\n",
        "  return outputs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cOiehqhQoiR7"
      },
      "source": [
        "url = \"https://s3.us-east-1.amazonaws.com/images.cocodataset.org/val2017/000000218091.jpg\"\n",
        "im2 = Image.open(requests.get(url, stream=True).raw)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wjSgz-nAihwY"
      },
      "source": [
        "outputs = plot_inference_segmentation(im2, \"bed\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vtRrvHtljTyy"
      },
      "source": [
        "outputs = plot_inference_segmentation(im2, \"phone\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LM6K8XzzpssF"
      },
      "source": [
        "outputs = plot_inference_segmentation(im2, \"lamp\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KBO0bHw9ylYR"
      },
      "source": [
        "## Question answering\n",
        "\n",
        "Finally, we demonstrate the inference on visual question answering. This model was trained on GQA, hence the questions must be similar (in particular, GQA doesn't contain counting questions)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6QODSn-WynTN"
      },
      "source": [
        "model_qa = torch.hub.load('ashkamath/mdetr:main', 'mdetr_efficientnetB5_gqa', pretrained=True, return_postprocessor=False)\n",
        "model_qa = model_qa.cuda()\n",
        "model_qa.eval();"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n_1tHurd5x4A"
      },
      "source": [
        "We download the mapping from the answers to their id."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iVcWecOC5esh"
      },
      "source": [
        "import json\n",
        "answer2id_by_type = json.load(requests.get(\"https://nyu.box.com/shared/static/j4rnpo8ixn6v0iznno2pim6ffj3jyaj8.json\", stream=True).raw)\n",
        "id2answerbytype = {}\n",
        "for ans_type in answer2id_by_type.keys():\n",
        "    curr_reversed_dict = {v: k for k, v in answer2id_by_type[ans_type].items()}\n",
        "    id2answerbytype[ans_type] = curr_reversed_dict"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cdzR45MUy5jC"
      },
      "source": [
        "def plot_inference_qa(im, caption):\n",
        "  # mean-std normalize the input image (batch-size: 1)\n",
        "  img = transform(im).unsqueeze(0).cuda()\n",
        "\n",
        "  # propagate through the model\n",
        "  memory_cache = model_qa(img, [caption], encode_and_save=True)\n",
        "  outputs = model_qa(img, [caption], encode_and_save=False, memory_cache=memory_cache)\n",
        "\n",
        "  # keep only predictions with 0.7+ confidence\n",
        "  probas = 1 - outputs['pred_logits'].softmax(-1)[0, :, -1].cpu()\n",
        "  keep = (probas > 0.7).cpu()\n",
        "\n",
        "  # convert boxes from [0; 1] to image scales\n",
        "  bboxes_scaled = rescale_bboxes(outputs['pred_boxes'].cpu()[0, keep], im.size)\n",
        "\n",
        "  # Extract the text spans predicted by each box\n",
        "  positive_tokens = (outputs[\"pred_logits\"].cpu()[0, keep].softmax(-1) > 0.1).nonzero().tolist()\n",
        "  predicted_spans = defaultdict(str)\n",
        "  for tok in positive_tokens:\n",
        "    item, pos = tok\n",
        "    if pos < 255:\n",
        "        span = memory_cache[\"tokenized\"].token_to_chars(0, pos)\n",
        "        predicted_spans [item] += \" \" + caption[span.start:span.end]\n",
        "\n",
        "  labels = [predicted_spans [k] for k in sorted(list(predicted_spans .keys()))]\n",
        "  plot_results(im, probas[keep], bboxes_scaled, labels)\n",
        "\n",
        "  # Classify the question type\n",
        "  type_conf, type_pred = outputs[\"pred_answer_type\"].softmax(-1).max(-1)\n",
        "  ans_type = type_pred.item()\n",
        "  types = [\"obj\", \"attr\", \"rel\", \"global\", \"cat\"]\n",
        "\n",
        "  ans_conf, ans = outputs[f\"pred_answer_{types[ans_type]}\"][0].softmax(-1).max(-1)\n",
        "  answer = id2answerbytype[f\"answer_{types[ans_type]}\"][ans.item()]\n",
        "  print(f\"Predicted answer: {answer}\\t confidence={round(100 * type_conf.item() * ans_conf.item(), 2)}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C1YpaycK0ST2"
      },
      "source": [
        "url = \"https://s3.us-east-1.amazonaws.com/images.cocodataset.org/val2017/000000076547.jpg\"\n",
        "im3 = Image.open(requests.get(url, stream=True).raw)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AuWXvsj73HvO"
      },
      "source": [
        "im3"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JQtxf_iD7oB3"
      },
      "source": [
        "During question answering, we can still plot the boxes that are detected by the model, thus giving us a hint of what's happening under the hood."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zKy3r0vt3IZy"
      },
      "source": [
        "plot_inference_qa(im3, \"What color is the train?\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WA5R24FZ3jy7"
      },
      "source": [
        "plot_inference_qa(im3, \"What is on the table?\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6zVLDmr23Psd"
      },
      "source": [
        "plot_inference_qa(im3, \"What is tied to the bike?\")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}